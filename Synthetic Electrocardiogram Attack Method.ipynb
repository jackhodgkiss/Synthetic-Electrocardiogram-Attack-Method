{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87750d47",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accb0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import bchlib\n",
    "import sqlite3\n",
    "import itertools\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from audiolazy import lazy_lpc\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import scipy.signal, scipy.fftpack\n",
    "from typing import Callable, Generator, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60f2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number = Union[int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc461c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_names = (*[name.strip('\\n') for name in open('datasets/mit-bih-normal-sinus-rhythm-database-1.0.0/RECORDS', 'r')],)\n",
    "last_beat_locations = (10247936, 10802693, 9516612, 9953468, 9740719, 10837734, 10002106, 10758639, 9566042, \n",
    "                    9779185, 9533735, 10355616, 9629397, 9294493, 8861963, 9246278, 9220939, 9723469)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28822865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Record:\n",
    "    \"\"\"\n",
    "    This class is responsible for wrapping around the wfdb.record object. It is designed to provide \n",
    "    easy access to the signal data and other useful parts of wfdb.record object.\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str, sample_from: int = 0, sample_to: Union[int, None] = None,\n",
    "                 channels: Union[list, None] = None, use_physical: bool = True):\n",
    "        \"\"\"\n",
    "        Construct a new Record object.\n",
    "        :param path: relative path to the record to be loaded.\n",
    "        :param sample_from: where should the sampling of the record begin from.\n",
    "        :param sample_to: where should the sample of the record end at.\n",
    "        :param channels: which channels should be loaded.\n",
    "        :param use_physical: should the phyiscal or digital values be loaded. \n",
    "        \"\"\"\n",
    "        self._path = path\n",
    "        self._sample_from = sample_from\n",
    "        self._sample_to = sample_to\n",
    "        self._channels = channels\n",
    "        self._use_physical = use_physical\n",
    "        self._data = wfdb.rdrecord(path, sample_from, sample_to, channels, use_physical)\n",
    "        \n",
    "    @property\n",
    "    def path(self) -> str:\n",
    "        return self._path\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.data.record_name\n",
    "    \n",
    "    @property\n",
    "    def sample_from(self) -> int:\n",
    "        return self._sample_from\n",
    "    \n",
    "    @property\n",
    "    def sample_to(self) -> int:\n",
    "        return self._sample_to\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        return self.data.sig_len\n",
    "    \n",
    "    @property\n",
    "    def channels(self) -> list:\n",
    "        return self._channels\n",
    "    \n",
    "    @property\n",
    "    def use_physical(self) -> bool:\n",
    "        return self._use_physical\n",
    "    \n",
    "    @property\n",
    "    def data(self) -> wfdb.Record:\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def channel_names(self) -> tuple:\n",
    "        return (*self.data.sig_name,)\n",
    "    \n",
    "    @property\n",
    "    def channel_count(self) -> int:\n",
    "        return len(self.channel_names)\n",
    "    \n",
    "    @property\n",
    "    def channel_units(self) -> tuple:\n",
    "        return (*self.data.units,)\n",
    "    \n",
    "    @property\n",
    "    def sample_rate(self) -> int:\n",
    "        return self.data.fs\n",
    "    \n",
    "    def extract_channel(self, channel_index: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract a single channel of either the physical or digital values from the record.\n",
    "        :param channel_index: which channel to extract.\n",
    "        :return: ndarray containing the extracted channel.\n",
    "        \"\"\"\n",
    "        result: Union[np.ndarray, None] = None\n",
    "        if self.use_physical:\n",
    "            try:\n",
    "                result = np.copy(self.data.p_signal[:, channel_index])\n",
    "            except IndexError as error:\n",
    "                raise error\n",
    "        else:\n",
    "            try:\n",
    "                result = np.copy(self.data.d_signal[:, channel_index])\n",
    "            except IndexError as error:\n",
    "                raise error\n",
    "        return result\n",
    "\n",
    "    def __getitem__(self, key: Union[str, int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Override the [] operator to allow for easy access when extracting the channels.\n",
    "        :param key: the index or the name of the channel to extract.\n",
    "        :return: ndarray containing the extracted channel.\n",
    "        \"\"\"\n",
    "        channel_index: int = self.channel_names.index(key) if isinstance(key, str) else key\n",
    "        return self.extract_channel(channel_index)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Human friendly representation of the class as a string.\"\"\"\n",
    "        return (f'Record Name: {self.name}, Record Path: {self.path}, Sample From: {self.sample_from}, '\n",
    "                f'Sample To: {self.sample_to}, Duration: {self.duration}, Channel Names: {self.channel_names}, '\n",
    "                f'Uses Physical: {self.use_physical}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae85ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code(Enum):\n",
    "    \"\"\"\n",
    "    This enum represent the possible annotations contained within the patient's annotation file.\n",
    "    \"\"\"\n",
    "    NORMAL:   str = 'N'\n",
    "    LBBB:     str = 'L'\n",
    "    RBBB:     str = 'R'\n",
    "    BBB:      str = 'B'\n",
    "    APC:      str = 'A'\n",
    "    ABERR:    str = 'a'\n",
    "    NPC:      str = 'J'\n",
    "    SVPB:     str = 'S'\n",
    "    PVC:      str = 'V'\n",
    "    RONT:     str = 'r'\n",
    "    FUSION:   str = 'F'\n",
    "    AESC:     str = 'e'\n",
    "    NESC:     str = 'j'\n",
    "    SVESC:    str = 'n'\n",
    "    VESC:     str = 'E'\n",
    "    PACE:     str = '/'\n",
    "    PFUS:     str = 'f'\n",
    "    UNKNOWN:  str = 'Q'\n",
    "    LEARN:    str = '?'\n",
    "    VFON:     str = '['\n",
    "    FLWAV:    str = '!'\n",
    "    VFOFF:    str = ']'\n",
    "    NAPC:     str = 'x'\n",
    "    WFON:     str = '('\n",
    "    WFOFF:    str = ')'\n",
    "    PWAVE:    str = 'p'\n",
    "    TWAVE:    str = 't'\n",
    "    UWAVE:    str = 'u'\n",
    "    PQ:       str = '‘'\n",
    "    JPT:      str = '’'\n",
    "    PACESP:   str = '^'\n",
    "    ARFCT:    str = '|'\n",
    "    NOISE:    str = '~'\n",
    "    RHYTHM:   str = '+'\n",
    "    STCH:     str = 's'\n",
    "    TCH:      str = 'T'\n",
    "    SYSTOLE:  str = '*'\n",
    "    DIASTOLE: str = 'D'\n",
    "    MEASURE:  str = '='\n",
    "    NOTE:     str = '\"'\n",
    "    LINK:     str = '@'\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Human friendly representation of the class as a string.\"\"\"\n",
    "        return f'{self.name} \\'{self.value}\\''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ef1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotations:\n",
    "    \"\"\"\n",
    "    This class is responsible for wrapping around the wfdb.annotations object.\n",
    "    It is designed to provide easy access to the annotations locations and their symbols.\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str, sample_from: int = 0, \n",
    "                 sample_to: Union[int, None] = None, extension: str = 'atr'):\n",
    "        \"\"\"\n",
    "        Construct a new Annotations object.\n",
    "        :param path: relative path to the record to be loaded.\n",
    "        :param sample_from: where should the sampling of the record begin from.\n",
    "        :param sample_to: where should the sample of the record end at.\n",
    "        :param extension: file extension of the annotation to be loaded.\n",
    "        \"\"\"\n",
    "        self._path = path\n",
    "        self._sample_from = sample_from\n",
    "        self._sample_to = sample_to\n",
    "        self._extension = extension\n",
    "        self._data = wfdb.rdann(path, extension, sample_from, sample_to)\n",
    "    \n",
    "    @property\n",
    "    def path(self):\n",
    "        return self._path\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.data.record_name\n",
    "    \n",
    "    @property \n",
    "    def sample_from(self) -> int:\n",
    "        return self._sample_from\n",
    "    \n",
    "    @property\n",
    "    def sample_to(self) -> int:\n",
    "        return self._sample_to\n",
    "    \n",
    "    @property\n",
    "    def extension(self) -> str:\n",
    "        return self._extension\n",
    "    \n",
    "    @property\n",
    "    def data(self) -> wfdb.Record:\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def locations(self) -> tuple:\n",
    "        return (*self.data.sample,)\n",
    "    \n",
    "    @property\n",
    "    def symbols(self) -> tuple:\n",
    "        return (*map(Code, self.data.symbol),)\n",
    "    \n",
    "    @property\n",
    "    def locations_and_symbols(self) -> zip:\n",
    "        return zip(self.locations, self.symbols)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Human friendly representation of the class as a string.\"\"\"\n",
    "        return (f'Record Name: {self.name}, Record Path: {self.path}, Extension: {self.extension}, '\n",
    "                f'Sample From: {self.sample_from}, Sample To: {self.sample_to}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7f88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_of_last_beat(path: str) -> Union[int, None]:\n",
    "    \"\"\"\n",
    "    Identify the location of the last normal beat within the specified patient record.\n",
    "    :param path: :param path: relative path to the record to be loaded.\n",
    "    :return: either None or the sample point at which the last normal beat occurred.\n",
    "    \"\"\"\n",
    "    loc_and_sym: tuple = (*Annotations(path).locations_and_symbols,)\n",
    "    for location, symbol in reversed(loc_and_sym):\n",
    "        if symbol is Code.NORMAL:\n",
    "            return int(location)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8df3b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter(data: np.ndarray, kernel_size: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a median filter against the supplied array of data. Can supply custom kernel size.\n",
    "    :param data: array of data to be filtered.\n",
    "    :param kernel_size: size of kernel or window that will slide across the data.\n",
    "    :return: new ndarray of filtered data. \n",
    "    \"\"\"\n",
    "    return scipy.signal.medfilt(data, kernel_size)\n",
    "\n",
    "\n",
    "def auto_correlation(data: np.ndarray, normalise: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the full auto correlation of the provided data. Will normalise the result by default.\n",
    "    See numpy.correlate for implementation details.\n",
    "    :param data: input data for which the auto correlation will computed over.\n",
    "    :param normalise: boolean to determine if the auto correlation should be returned normalised.\n",
    "    :return: auto correlation of the provided data array.\n",
    "    \"\"\"\n",
    "    coefficients = np.correlate(data, data, 'full')\n",
    "    return coefficients / coefficients[data.size - 1] if normalise == True else coefficients\n",
    "\n",
    "\n",
    "def cosine_transformation(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform the discrete cosine transformation over the provided data.\n",
    "    See scipy.fftpack.dct for implementation details. Custom scaling has been applied.\n",
    "    See http://cs.uccs.edu/~cs525/video/dctKhayam.pdf for equation definition.\n",
    "    :param data: input data for which the dct will be computed for.\n",
    "    :return: dct coefficients.\n",
    "    \"\"\"\n",
    "    coefficients = scipy.fftpack.dct(data) / 2\n",
    "    for index, coefficient in enumerate(coefficients):\n",
    "        if index == 0:\n",
    "            coefficients[index] = np.sqrt(1 / data.size) * coefficient\n",
    "        else:\n",
    "            coefficients[index] = np.sqrt(2 / data.size) * coefficient\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b73416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signal_features(signal: np.ndarray) -> tuple[list, list, list]:\n",
    "    \"\"\"\n",
    "    Extract the signal features as defined by the ELPA key generation and agreement scheme.\n",
    "    :param signal: signal which contains features to be extracted.\n",
    "    :return: tuple containing extracted signal features. \n",
    "    \"\"\"\n",
    "    correlation_coefficients = auto_correlation(signal)\n",
    "    l_coefficients = correlation_coefficients[signal.size - 1:]\n",
    "    consine_coefficients = cosine_transformation(l_coefficients)\n",
    "    return (correlation_coefficients, l_coefficients, consine_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85b8be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_consine_coefficients(cosine_coefficients: np.ndarray, prediction_coefficients: Optional[np.ndarray] = None) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Predict the cosine coefficients using the linear prediction coding method. \n",
    "    If no prediction cofficients are provided then some are generated and returned.\n",
    "    :param cosine_coefficients: cosine coefficients to be predicted.\n",
    "    :param prediction_coefficients: optional linear predicition coefficients that.\n",
    "    :return: both the prediction result and the prediction coefficients used.\n",
    "    \"\"\"\n",
    "    if not prediction_coefficients:\n",
    "        prediction_coefficients = lazy_lpc.lpc(cosine_coefficients, order=4).zeros\n",
    "    prediction = abs(scipy.signal.lfilter(prediction_coefficients, [1], cosine_coefficients))\n",
    "    return prediction, prediction_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfe70001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prediction_into_key(original: np.ndarray, prediction: np.ndarray) -> tuple[tuple, tuple]:\n",
    "    \"\"\"\n",
    "    Convert the prediction into a key using the pulse code train.\n",
    "    :param original: the original cosine coefficient array.\n",
    "    :param prediction: the predicted cosine coefficient array.\n",
    "    :return: tuple containing key encoded as binary (integer either 1 or 0) and as the errors that is originates from. \n",
    "    \"\"\"\n",
    "    calculate_errors = lambda original_element, prediction_element: original_element - prediction_element\n",
    "    pulse_code_train = lambda err: 1 if err >= 0 else 0\n",
    "    errors = (*map(calculate_errors, original, prediction),)\n",
    "    key = (*(map(pulse_code_train, errors)),)\n",
    "    return (key, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee87f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_key_agreement(lead_i: np.ndarray, lead_ii: np.ndarray, bch, kernel_size: int = 9) -> tuple[...]:\n",
    "    \"\"\"\n",
    "    Perform the ELPA key generation and agreement method.\n",
    "    :param lead_i: ECG data originating from the 'sender' in the scheme or alice.\n",
    "    :param lead_ii: ECG data originating from the 'receiver' in this scheme or bob'.\n",
    "    :param bch: BCH lib instance.\n",
    "    :param kernel_size: the size of kernel used in the median filter.\n",
    "    :return: tuple containing key, lead_ii errors and lead_ii bit flips. \n",
    "    \"\"\"\n",
    "    lead_i = median_filter(lead_i, kernel_size)\n",
    "    lead_ii = median_filter(lead_ii, kernel_size)\n",
    "    lead_i_features = extract_signal_features(lead_i)\n",
    "    lead_ii_features = extract_signal_features(lead_ii)\n",
    "    lead_i_prediction, prediction_coefficients = predict_consine_coefficients(lead_i_features[-1])\n",
    "    lead_ii_prediction, _ = predict_consine_coefficients(lead_ii_features[-1], prediction_coefficients)\n",
    "    lead_i_key, lead_i_key_errors = convert_prediction_into_key(lead_i_features[-1][:128], lead_i_prediction[:128])\n",
    "    lead_ii_key, lead_ii_key_errors = convert_prediction_into_key(lead_ii_features[-1][:128], lead_ii_prediction[:128])\n",
    "    lead_i_key_as_bytes = bytearray(lead_i_key)\n",
    "    lead_ii_key_as_bytes = bytearray(lead_ii_key)\n",
    "    error_correction_codes = bch.encode(lead_i_key_as_bytes)\n",
    "    lead_ii_bit_flips, _, _ = bch.decode(lead_ii_key_as_bytes, error_correction_codes)\n",
    "    return (lead_i_key, lead_i_key_errors, error_correction_codes, prediction_coefficients, lead_ii_key, lead_ii_key_errors, lead_ii_bit_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbb361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_attack(bch, error_correction_codes, prediction_coefficients, attack_signal: np.ndarray, kernel_size: int = 9) -> tuple[...]:\n",
    "    attack_signal = median_filter(attack_signal, kernel_size)\n",
    "    attack_features = extract_signal_features(attack_signal)\n",
    "    attack_predictions, _ = predict_consine_coefficients(attack_features[-1], prediction_coefficients)\n",
    "    attack_key, attack_errors = convert_prediction_into_key(attack_features[-1][:128], attack_predictions[:128])\n",
    "    attack_key_as_bytes = bytearray(attack_key)\n",
    "    attack_bit_flips, _, _ = bch.decode(attack_key_as_bytes, error_correction_codes)\n",
    "    return (attack_key, attack_errors, attack_bit_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5dae0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_normal_beats(annotations: Annotations, sample_from: int, sample_to: int, skew_offset: int = 2) -> tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Identify all of the normal beats within the patient record file.\n",
    "    :param annotations: annotations which contain the location of all beats normal or otherwise.\n",
    "    :param sample_from: where should the sampling of the record begin from.\n",
    "    :param sample_to: where should the sample of the record end at.\n",
    "    :param skew_offset: apply offset to compensate for skew between record and annotation.\n",
    "    :return: tuple containing all of the located normal beats.\n",
    "    \"\"\"\n",
    "    return (*((location + skew_offset) - sample_from for location, code \n",
    "              in annotations.locations_and_symbols if location >= sample_from and location <= sample_to and code is Code.NORMAL),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba3ced95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(annotations: Annotations, sample_from: int, sample_to: int) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if the provided range of samples is valid for use within the experiments.\n",
    "    :param annotations: annotations which contain the location of all beats normal or otherwise.\n",
    "    :param sample_from: where should the sampling of the record begin from.\n",
    "    :param sample_to: where should the sample of the record end at.\n",
    "    :return: bool stating if it is valid of not. \n",
    "    \"\"\"\n",
    "    locations = [location for location in annotations.locations if location >= sample_from and location <= sample_to]\n",
    "    symbols = [symbol for location, symbol in annotations.locations_and_symbols if location in locations]\n",
    "    return all(symbol == Code.NORMAL for symbol in symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ffb09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_compl(signal: np.ndarray, location_of_normal_beat: tuple, n_samples: int = 30) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract a single QRS complex that can identified from the normal beat locations.\n",
    "    :param signal: the signal data in which the QRS complex shall be extracted from.\n",
    "    :param location_of_normal_beat: tuple containing the location of normal beats within the signal.\n",
    "    :param n_samples: number of samples either side of the normal beat be extracted as a single complex.\n",
    "    :return: a new ndarray of containing the samples that make up that complex.\n",
    "    \"\"\"\n",
    "    start, end = location_of_normal_beat - n_samples, location_of_normal_beat + n_samples\n",
    "    if start < 0 or end >= len(signal):\n",
    "        return None\n",
    "    identify_peak = lambda : np.argmax(signal[start:end])\n",
    "    peak_location = start + identify_peak()\n",
    "    start, end = peak_location - n_samples, peak_location + n_samples\n",
    "    return signal[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08311c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_compls(signal: np.ndarray, location_of_normal_beats: tuple, n_samples: int = 30) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract all of the QRS complexes within the provided signal.\n",
    "    :param signal: the signal data in which the QRS complexes shall be extracted from.\n",
    "    :param location_of_normal_beat: tuple containing the location of normal beats within the signal.\n",
    "    :param n_samples: number of samples either side of the normal beat be extracted as a single complex.\n",
    "    :return: a new ndarray of containing all of the QRS complexes.\n",
    "    \"\"\"\n",
    "    compls = [extract_compl(signal, location, n_samples) for location in location_of_normal_beats]\n",
    "    valid_compls = [*filter(lambda compl : compl is not None, compls)]\n",
    "    return ([*filter(lambda compl : compl.size == 2 * n_samples, valid_compls)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dddc2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_power_indices(n: int, n_symbols: int, order: int) -> tuple[int, ...]:\n",
    "    result = [0] * order\n",
    "    result[0], remainder = divmod(n, n_symbols ** (order - 1))\n",
    "    for counter in range(order - 2, -1, -1):\n",
    "        result[order - counter - 1], remainder = divmod(remainder, n_symbols ** counter)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a73e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_power_index(cartesian_power: tuple[int, ...], symbols: tuple[int, ...]) -> int:\n",
    "    result = 0\n",
    "    order, n_symbols = len(cartesian_power), len(symbols)\n",
    "    power_indices = [symbols.index(element) for element in cartesian_power]\n",
    "    for counter in range(order - 1, -1, -1):\n",
    "        result += power_indices[order - counter - 1] * n_symbols ** counter\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6743464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_power_n(n: int, symbols: tuple[int, ...], order: int) -> tuple[int, ...]:\n",
    "    return (*(symbols[index] for index in cartesian_power_indices(n, len(symbols), order)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f2cccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(symbols: tuple[int, ...], order: int) -> Generator[tuple[int, ...], None, None]:\n",
    "    for counter in range(len(symbols) ** order):\n",
    "        yield cartesian_power_n(counter, symbols, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a99284d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(data: list) -> zip:\n",
    "    a, b = itertools.tee(data)\n",
    "    next(b, None)\n",
    "    return zip(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bf181d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_between_peaks(peak_locations: tuple[int, ...]) -> tuple[int, ...]:\n",
    "    return [b - a for a, b in pairwise(peak_locations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e189b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_to_peaks(distances: tuple[int, ...]) -> tuple[int, ...]:\n",
    "    result = [35]\n",
    "    for distance in distances:\n",
    "        result.append(result[-1] + distance)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e002d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_signal(segment: np.ndarray, peak_locations: tuple, signal_length: int = 640) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Construct a signal using a QRS complex and a tuple of peak locations where the complexes shall be placed.\n",
    "    :param segment: single QRS complex segment.\n",
    "    :param peak_locations: tuple containing the location where peaks should be placed.\n",
    "    :param signal_length: max size of the constructed signal.\n",
    "    \"\"\"\n",
    "    signal = [None] * signal_length\n",
    "    segment_size = len(segment)\n",
    "    peak_offset = segment_size // 2\n",
    "    for peak_location in peak_locations:\n",
    "        start, end = peak_location - peak_offset, peak_location + peak_offset\n",
    "        if start != abs(start):\n",
    "            n_samples_discarded = abs(start)\n",
    "            signal[:end] = segment[n_samples_discarded:]\n",
    "        else:\n",
    "            signal[start:end] = segment\n",
    "    signal = [0 if sample is None else sample for sample in signal]\n",
    "    return np.asarray(signal[:signal_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1fa4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_connection(database_path: str = 'results.db') -> sqlite3.Connection:\n",
    "    \"\"\"\n",
    "    This function shall open a connection between the database and the application.\n",
    "    :param database_path: path to the database.\n",
    "    :return: sqlite3 connection object that represents the connection between the database and the application.\n",
    "    \"\"\"\n",
    "    database_connection = None\n",
    "    try:\n",
    "        database_connection = sqlite3.connect(database_path)\n",
    "    except sqlite3.Error as error:\n",
    "        print(error)\n",
    "    return database_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b393485",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KeyMetaData:\n",
    "    ID: int\n",
    "    KeyInBits: str\n",
    "    KeyErrors: tuple\n",
    "    KeyBitFlips: int\n",
    "\n",
    "    def insert(self, database_connection: sqlite3.Connection):\n",
    "        cursor = database_connection.cursor()\n",
    "        cursor.execute('INSERT INTO KeyMetaData (KeyInBits, KeyErrors, KeyBitFlips) VALUES (?, ?, ?)', \n",
    "            (str(self.KeyInBits), str(self.KeyErrors), self.KeyBitFlips))\n",
    "        self.ID = cursor.lastrowid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0878f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KeyAgreementInstance:\n",
    "    ID: int\n",
    "    SampleFrom: int\n",
    "    K1MetaData: KeyMetaData\n",
    "    K2MetaData: KeyMetaData\n",
    "    PredictionCoefficients: tuple\n",
    "    BCHCoefficients: tuple\n",
    "    TargetPeakLocations: tuple\n",
    "\n",
    "    def insert(self, database_connection: sqlite3.Connection):\n",
    "        cursor = database_connection.cursor()\n",
    "        cursor.execute('INSERT INTO KeyAgreementInstance (SampleFrom, K1MetaDataID, K2MetaDataID, PredictionCoefficients, BCHCoefficients, TargetPeakLocations) VALUES (?, ?, ?, ?, ?, ?)', \n",
    "            (self.SampleFrom, self.K1MetaData.ID, self.K2MetaData.ID, str(self.PredictionCoefficients), str(self.BCHCoefficients), str(self.TargetPeakLocations)))\n",
    "        self.ID = cursor.lastrowid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "866fbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experiment:\n",
    "    ID: int\n",
    "    RecordName: str\n",
    "    _KeyAgreementInstance: KeyAgreementInstance\n",
    "\n",
    "    def insert(self, database_connection: sqlite3.Connection):\n",
    "        cursor = database_connection.cursor()\n",
    "        RecordID = cursor.execute('SELECT ID FROM Record WHERE Name=?', (self.RecordName,)).fetchone()[0]\n",
    "        cursor.execute('INSERT INTO Experiment (RecordID, ExperimentParametersID, KeyAgreementInstanceID) VALUES (?, ?, ?)', \n",
    "            (RecordID, 1, self._KeyAgreementInstance.ID))\n",
    "        self.ID = cursor.lastrowid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c99af082",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Attack:\n",
    "    ID: int\n",
    "    _Experiment: Experiment\n",
    "    OffsetAsSeconds: int\n",
    "    K3MetaData: KeyMetaData\n",
    "    K4MetaData: KeyMetaData\n",
    "\n",
    "    def insert(self, database_connection: sqlite3.Connection):\n",
    "        cursor = database_connection.cursor()\n",
    "        cursor.execute('INSERT INTO Attack (AttackParametersID, ExperimentID, LeadName, OffsetAsSeconds, K3MetaDataID, K4MetaDataID) VALUES (?, ?, ?, ?, ?, ?)', \n",
    "            (1, self._Experiment.ID, 'ECG1', self.OffsetAsSeconds, self.K3MetaData.ID, self.K4MetaData.ID))\n",
    "        self.ID = cursor.lastrowid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fddb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Attempt:\n",
    "    ID: int\n",
    "    _Attack: Attack\n",
    "    K5MetaData: KeyMetaData\n",
    "    PeakPositions: tuple\n",
    "\n",
    "    def insert(self, database_connection: sqlite3.Connection):\n",
    "        cursor = database_connection.cursor()\n",
    "        cursor.execute('INSERT INTO Attempt (AttackID, K5MetaDataID, PeakPositions) VALUES (?, ?, ?)', \n",
    "            (self._Attack.ID, self.K5MetaData.ID, str(self.PeakPositions)))\n",
    "        self.ID = cursor.lastrowid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f7317cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results():\n",
    "    def __init__(self):\n",
    "        self._K1MetaData = None\n",
    "        self._K2MetaData = None\n",
    "        self._KeyAgreementInstance = None\n",
    "        self._Experiment = None\n",
    "        self._K3MetaData = {}\n",
    "        self._K4MetaData = {}\n",
    "        self._Attacks = {}\n",
    "        self._K5MetaData = {}\n",
    "        self._Attempts = {}\n",
    "\n",
    "    def insert_results(self, database_connection: sqlite3.Connection):\n",
    "        self._K1MetaData.insert(database_connection)\n",
    "        self._K2MetaData.insert(database_connection)\n",
    "        self._KeyAgreementInstance.insert(database_connection)\n",
    "        self._Experiment.insert(database_connection)\n",
    "        for offset in self._Attacks.keys():\n",
    "            self._K3MetaData[offset].insert(database_connection)\n",
    "            self._K4MetaData[offset].insert(database_connection)\n",
    "            self._Attacks[offset].insert(database_connection)\n",
    "            for index, _ in enumerate(self._Attempts[offset]):\n",
    "                self._K5MetaData[offset][index].insert(database_connection)\n",
    "                self._Attempts[offset][index].insert(database_connection)\n",
    "        database_connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3aa96d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(patient_record: Record, patient_annotations: Annotations, sample_from: int = 76800, duration: int = 640, adversary_offset: int = [7680], kernel_size: int = 9, bch_bits: int = 128):\n",
    "    results = None\n",
    "    if is_valid(patient_annotations, sample_from, sample_from + duration):\n",
    "        results = Results()\n",
    "        bch = bchlib.BCH(8219, bch_bits)\n",
    "        lead_i, lead_ii = patient_record['ECG1'][sample_from:sample_from + duration], patient_record['ECG2'][sample_from:sample_from + duration]\n",
    "        lead_i_key, lead_i_key_errors, error_correction_codes, prediction_coefficients, lead_ii_key, lead_ii_key_errors, lead_ii_bit_flips = perform_key_agreement(lead_i, lead_ii, bch, kernel_size)\n",
    "        lead_i_beat_locations = locate_normal_beats(patient_annotations, sample_from, sample_from + duration)\n",
    "        lead_i_beat_distances = distance_between_peaks(lead_i_beat_locations)\n",
    "        lead_i_beat_elements = [*range(min(lead_i_beat_distances), max(lead_i_beat_distances) + 1),]\n",
    "        lead_i_n_beats = len(lead_i_beat_locations)\n",
    "        target_peak_index = cartesian_power_index(lead_i_beat_distances, lead_i_beat_elements)\n",
    "        results._K1MetaData = KeyMetaData(0, lead_i_key, None, None)\n",
    "        results._K2MetaData = KeyMetaData(0, None, None, lead_ii_bit_flips)\n",
    "        results._KeyAgreementInstance = KeyAgreementInstance(0, sample_from, results._K1MetaData, results._K2MetaData, None, None, lead_i_beat_locations)\n",
    "        results._Experiment = Experiment(0, patient_record.name, results._KeyAgreementInstance)\n",
    "        for offset in adversary_offset:\n",
    "            if is_valid(patient_annotations, sample_from - offset - duration, sample_from - offset):\n",
    "                adversary_lead = patient_record['ECG1'][sample_from - offset - duration:sample_from - offset]\n",
    "                adversary_lead_key, adversary_lead_errors, adversary_lead_bit_flips = perform_attack(bch, error_correction_codes, prediction_coefficients, adversary_lead, kernel_size)\n",
    "                adversary_lead_beat_locations = locate_normal_beats(patient_annotations, sample_from - offset - duration, sample_from - offset)\n",
    "                if adversary_lead_beat_locations:\n",
    "                    selected_compl = extract_all_compls(adversary_lead, adversary_lead_beat_locations)[0]\n",
    "                    perfect_synthetic_signal = construct_signal(selected_compl, lead_i_beat_locations)\n",
    "                    perfect_synthetic_key, perfect_synthetic_errors, perfect_synthetic_bit_flips = perform_attack(bch, error_correction_codes, prediction_coefficients, perfect_synthetic_signal, kernel_size)\n",
    "                    results._K3MetaData[offset] = KeyMetaData(0, None, None, adversary_lead_bit_flips)\n",
    "                    results._K4MetaData[offset] = KeyMetaData(0, None, None, perfect_synthetic_bit_flips)\n",
    "                    results._Attacks[offset] = Attack(0, results._Experiment, offset, results._K3MetaData[offset], results._K4MetaData[offset])\n",
    "                    K5s = []\n",
    "                    attempts = []\n",
    "                    attempts_lower_n = target_peak_index - 500 if target_peak_index > 500 else 0\n",
    "                    attempts_upper_n = target_peak_index + 500 if len(lead_i_beat_elements) ** lead_i_n_beats - target_peak_index > 500 else len(lead_i_beat_elements) ** lead_i_n_beats\n",
    "                    for counter, n in enumerate(range(attempts_lower_n, attempts_upper_n)):\n",
    "                        synthetic_peak_distances = cartesian_power_n(n, lead_i_beat_elements, lead_i_n_beats)\n",
    "                        synthetic_peak_locations = distances_to_peaks(synthetic_peak_distances)\n",
    "                        synthetic_signal = construct_signal(selected_compl, synthetic_peak_locations)\n",
    "                        synthetic_key, synthetic_errors, synthetic_bit_flips = perform_attack(bch, error_correction_codes, prediction_coefficients, synthetic_signal, kernel_size)\n",
    "                        K5s.append(KeyMetaData(0, None, None, synthetic_bit_flips))\n",
    "                        attempts.append(Attempt(0, results._Attacks[offset], K5s[counter], synthetic_peak_locations))\n",
    "                    results._K5MetaData[offset] = K5s\n",
    "                    results._Attempts[offset] = attempts\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "451f6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_record = Record(f'datasets/mit-bih-normal-sinus-rhythm-database-1.0.0/{record_names[0]}')\n",
    "patient_annotations = Annotations(f'datasets/mit-bih-normal-sinus-rhythm-database-1.0.0/{record_names[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a08e3787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 1\n",
      "Patient Record: 16265\n",
      "Sample From: 153600\n",
      "Target Peaks: (8, 93, 178, 262, 347, 431, 520, 613)\n",
      "Adversary Errors Without SEAM: 58\n",
      "Adversary Errors With SEAM: 17\n",
      "\n",
      "Experiment: 2\n",
      "Patient Record: 16265\n",
      "Sample From: 161280\n",
      "Target Peaks: (59, 149, 238, 327, 416, 503, 591)\n",
      "Adversary Errors Without SEAM: 29\n",
      "Adversary Errors With SEAM: 14\n",
      "\n",
      "Experiment: 3\n",
      "Patient Record: 16265\n",
      "Sample From: 168960\n",
      "Target Peaks: (54, 139, 222, 307, 391, 478, 563)\n",
      "Adversary Errors Without SEAM: 27\n",
      "Adversary Errors With SEAM: 21\n",
      "\n",
      "Experiment: 4\n",
      "Patient Record: 16265\n",
      "Sample From: 176640\n",
      "Target Peaks: (37, 126, 214, 299, 387, 479, 571)\n",
      "Adversary Errors Without SEAM: 58\n",
      "Adversary Errors With SEAM: 25\n",
      "\n",
      "Experiment: 5\n",
      "Patient Record: 16265\n",
      "Sample From: 184320\n",
      "Target Peaks: (10, 90, 171, 251, 333, 418, 503, 590)\n",
      "Adversary Errors Without SEAM: 47\n",
      "Adversary Errors With SEAM: 19\n",
      "\n",
      "Experiment: 6\n",
      "Patient Record: 16265\n",
      "Sample From: 192000\n",
      "Target Peaks: (38, 125, 212, 299, 384, 473, 558, 642)\n",
      "Adversary Errors Without SEAM: 65\n",
      "Adversary Errors With SEAM: 26\n",
      "\n",
      "Experiment: 7\n",
      "Patient Record: 16265\n",
      "Sample From: 199680\n",
      "Target Peaks: (24, 115, 209, 305, 405, 501, 597)\n",
      "Adversary Errors Without SEAM: 69\n",
      "Adversary Errors With SEAM: 24\n",
      "\n",
      "Experiment: 8\n",
      "Patient Record: 16265\n",
      "Sample From: 207360\n",
      "Target Peaks: (48, 135, 226, 321, 417, 512, 608)\n",
      "Adversary Errors Without SEAM: 43\n",
      "Adversary Errors With SEAM: 32\n",
      "\n",
      "Experiment: 9\n",
      "Patient Record: 16265\n",
      "Sample From: 215040\n",
      "Target Peaks: (35, 127, 217, 309, 400, 494, 589)\n",
      "Adversary Errors Without SEAM: 54\n",
      "Adversary Errors With SEAM: 15\n",
      "\n",
      "Experiment: 10\n",
      "Patient Record: 16265\n",
      "Sample From: 222720\n",
      "Target Peaks: (60, 148, 240, 331, 422, 512, 604)\n",
      "Adversary Errors Without SEAM: 54\n",
      "Adversary Errors With SEAM: 23\n",
      "\n",
      "Experiment: 11\n",
      "Patient Record: 16265\n",
      "Sample From: 230400\n",
      "Target Peaks: (27, 115, 200, 294, 384, 482, 575)\n",
      "Adversary Errors Without SEAM: 60\n",
      "Adversary Errors With SEAM: 27\n",
      "\n",
      "Experiment: 12\n",
      "Patient Record: 16265\n",
      "Sample From: 238080\n",
      "Target Peaks: (25, 116, 210, 302, 394, 486, 584)\n",
      "Adversary Errors Without SEAM: 59\n",
      "Adversary Errors With SEAM: 24\n",
      "\n",
      "Experiment: 13\n",
      "Patient Record: 16265\n",
      "Sample From: 245760\n",
      "Target Peaks: (7, 95, 181, 267, 357, 446, 534, 624)\n",
      "Adversary Errors Without SEAM: 40\n",
      "Adversary Errors With SEAM: 24\n",
      "\n",
      "Experiment: 14\n",
      "Patient Record: 16265\n",
      "Sample From: 253440\n",
      "Target Peaks: (44, 133, 224, 313, 401, 492, 583)\n",
      "Adversary Errors Without SEAM: 43\n",
      "Adversary Errors With SEAM: 21\n",
      "\n",
      "Experiment: 15\n",
      "Patient Record: 16265\n",
      "Sample From: 261120\n",
      "Target Peaks: (48, 146, 242, 334, 422, 508, 596)\n",
      "Adversary Errors Without SEAM: 52\n",
      "Adversary Errors With SEAM: 30\n",
      "\n",
      "Experiment: 16\n",
      "Patient Record: 16265\n",
      "Sample From: 268800\n",
      "Target Peaks: (51, 139, 226, 312, 399, 488, 580)\n",
      "Adversary Errors Without SEAM: 40\n",
      "Adversary Errors With SEAM: 14\n",
      "\n",
      "Experiment: 17\n",
      "Patient Record: 16265\n",
      "Sample From: 276480\n",
      "Target Peaks: (80, 175, 266, 357, 447, 542, 635)\n",
      "Adversary Errors Without SEAM: 67\n",
      "Adversary Errors With SEAM: 37\n",
      "\n",
      "Experiment: 18\n",
      "Patient Record: 16265\n",
      "Sample From: 284160\n",
      "Target Peaks: (48, 138, 228, 318, 408, 502, 593)\n",
      "Adversary Errors Without SEAM: 51\n",
      "Adversary Errors With SEAM: 24\n",
      "\n",
      "Experiment: 19\n",
      "Patient Record: 16265\n",
      "Sample From: 291840\n",
      "Target Peaks: (38, 126, 214, 304, 391, 479, 571)\n",
      "Adversary Errors Without SEAM: 36\n",
      "Adversary Errors With SEAM: 22\n",
      "\n",
      "Experiment: 20\n",
      "Patient Record: 16265\n",
      "Sample From: 299520\n",
      "Target Peaks: (38, 126, 213, 303, 391, 478, 566)\n",
      "Adversary Errors Without SEAM: 35\n",
      "Adversary Errors With SEAM: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, sample_from in enumerate(range(153600, 307200, 7680), 1):\n",
    "    results = perform_experiment(patient_record, patient_annotations, sample_from)\n",
    "    print(f'Experiment: {index}\\n'\n",
    "      f'Patient Record: {patient_record.name}\\n' \n",
    "      f'Sample From: {sample_from}\\n' \n",
    "      f'Target Peaks: {results._KeyAgreementInstance.TargetPeakLocations}\\n'\n",
    "      f'Adversary Errors Without SEAM: {results._K3MetaData[7680].KeyBitFlips}\\n'\n",
    "      f'Adversary Errors With SEAM: {results._K4MetaData[7680].KeyBitFlips}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47cdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
